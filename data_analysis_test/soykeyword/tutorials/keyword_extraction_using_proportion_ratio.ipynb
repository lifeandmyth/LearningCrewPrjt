{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config는 실험용 데이터의 path를 저장한 파일이므로, 여러분의 코퍼스의 위치를 지정하시면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_corpus_fname과 tokenized_corpus_fname의 타입은 str입니다. \n",
      "<class 'str'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from config import raw_corpus_fname, tokenized_corpus_fname\n",
    "print('raw_corpus_fname과 tokenized_corpus_fname의 타입은 str입니다. ')\n",
    "print(type(raw_corpus_fname), type(tokenized_corpus_fname))\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import soykeyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-10-20_article_all_normed_nountokenized.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_fname.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion ratio를 이용하는 키워드 추출 방법은 CorpusbasedKeywordExtractor와 MatrixbasedKeywordExtractor 두 종류로 구현되어 있습니다. CorpusbasedKeywordExtractor는 아래 예시처럼 text 파일에서 키워드를 직접 추출하는 코드이며, MatrixbasedKeywordExtractor는 sparse format으로 만들어둔 term frequency matrix에서 키워드를 추출하는 코드입니다. \n",
    "\n",
    "Proportion ratio의 개념은 아래와 같습니다. \n",
    "\n",
    "키워드란 사실 명확한 정의가 있는 단어가 아닙니다. 흔히 사용하는 키워드의 정의에는 자주 나오는 단어나, TF-IDF 값이 있습니다. 이들은 각자의 관점으로 키워드를 정의한 것입니다. 자주 나오는 단어를 키워드로 정의하는 것은 많이 나올수록 키워드라는 의미이며, 이 때에는 조사와 같은 단어가 키워드가 될 수도 있습니다. 이를 보완하기 위해 품사 판별 (Part-of-Speech tagging)을 한 뒤, 명사만을 추출하여 최빈어를 키워드로 선정하는 것은 합리적이라 생각됩니다. TF-IDF를 키워드로 사용하는 방법은 조금 위험한 방법입니다. IDF는 단어가 등장한 문서의 개수가 적을수록 커지기 때문에 오히려 노이즈일 가능성이 높기 때문입니다. \n",
    "\n",
    "그 외에도 chi-square를 이용하는 방법도 있습니다. 제가 제안하는 Proportion ratio는 이 방법에 가깝습니다. 기본 컨셉은 아래와 같습니다. '뉴스에서의 키워드'를 선택하라는 말은 애매모호합니다. 하지만, '오늘의 뉴스에서의 키워드'나 '아이오아이에 대한 문서에서의 키워드'라는 말은 조금 더 명료합니다. 키워드에 대한 관점이 생기기 때문입니다. 좀 더 자세히, 여름철 뉴스에서는 '호우'라는 단어가 0.1% 씩 늘 등장한다고 가정합시다. 어느날 '호우'라는 단어가 평상시와 다르게 0.9% 등장하였다면 (평상시보다 9배), 이 날은 정말로 호우가 내려서 뉴스에 그 단어가 자주 등장했을 가능성이 높습니다. 그렇다면 '호우'는 그날의 키워드가 될 수 있을 것입니다. 이를 수치로 만들기 위해서 다음과 같은 지표를 만들었습니다. \n",
    "\n",
    "    score(w) = P(w|Dt) / { P(w|Dt) + P(w|Dr) }\n",
    "    \n",
    "    P(w|Dt): target document에서 단어 w가 출현한 비율\n",
    "    P(w|Dr): reference document에서 단어 w가 출현한 비율\n",
    "\n",
    "Target document란 키워드를 정의하고 싶은 문서 집합을 의미합니다. '아이오아이라는 단어가 포함된 뉴스'라던가, 어느날의 뉴스가 됩니다. Reference document는 평상시의 문서 집합입니다. '아이오아이'를 포함한 연예뉴스 가 될 수도, 하루치 전체 뉴스가 될 수도 있습니다. 어느 하루의 뉴스의 키워드를 선택하기 위해서는 이전 10일치의 뉴스를 reference document로 선택할 수 있습니다. \n",
    "\n",
    "이렇게 keyword score를 정의하면 score(w)는 [0, 1] 사이의 값이 됩니다. 평상시 호우가 0.1% 등장하였다가 오늘 0.9% 등장하였다면 score는 0.9 / (0.1 + 0.9) = 0.9 입니다. 평상시와 같이 0.1% 등장하였다면 (0.1 / (0.1 + 0.1)) = 0.5가 됩니다. 0.5란 평상시와 다르지 않다는 의미이며, 그 이하는 평상시보다 등장하지 않았다는 의미입니다. 하지만 0.5보다 작은 값은 의미가 없습니다. target document set은 reference document set보다 훨씬 작은 집합이기 때문에 많은 단어를 포함하지 않을 수 있기 때문입니다. 대신 0.5보다 큰 score를 지니는 단어들은 평상시보다 자주 등장한 단어임을 의미합니다. 이때에는 한가지 false alarm이 생길 수 있습니다. 애초에 자주 등장하지 않는 단어이기 때문에 target documents에만 등장하는 단어는 1.0에 가까운 score를 가지게 됩니다. 이를 방지하기 위해서 최소한 등장해야 하는 단어 빈도수를 한정할 필요가 있습니다. 그래서 키워드를 선택할 때 항상 parameter로 min_frequency를 넣도록 하였습니다. \n",
    "\n",
    "    keywords = corpusbased_extractor.extract_from_word('아이오아이', min_score=0.8, min_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        self.length = 0\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip()\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            with open(self.fname, encoding='utf-8') as f:\n",
    "                for n_doc, _ in enumerate(f):\n",
    "                    continue\n",
    "                self.length = (n_doc + 1)\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial에 올리는 파일은 2016-10-20의 하루치 뉴스를 크롤링한 데이터입니다. 데이터는 공유할 수 없음을 양해 부탁드립니다. 사용하는 데이터 포멧은 한 줄이 하나의 뉴스에 해당합니다. 하루치 뉴스에 대하여 명사를 추출한 뒤, 이를 list 형태로 저장하고 있습니다. doc.split()을 하면 명사들이 return 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc=6, num words=187, 1억 달러 리스크 강화 뉴욕 연합뉴스 특파원 미국 대형은행 골드만삭스 근무 트레이더 투자 등급 채권 정크본드 투자 사이 1억 달러 1천 이익 것으로 금융 글로벌 금융위기 이후 대형은행 리스크 강조 상황 이런 대박 않은 것으로 월스트리트저널 뉴욕 고수익 골드만삭스 34 관리 정크본드 투자 1억 달러 이상 수익 골드만삭스 소식통 인용 19일 현지시간 보도 1월\n",
      "\n",
      "doc=7, num words=25, 서울 연합뉴스 19일 서울 시내 출동 경찰관 사제 총기 발사 용의자 범행 사회관계망서비스 용의자 경찰 내게 살인 누명 경찰 적대감 차례 2016 페이스북 캡처 연합뉴스\n",
      "\n",
      "doc=8, num words=271, 제안 한미 해군 수상전센터 미래 해상 연구 워싱턴 연합뉴스 이영 기자 제4 한미안보협의회 참석 미국 방문 한민구 국방부 장관 19일 현지시간 해군 최첨단 무기체계 개발 수상전센터 방문 국방부 관계자는 이날 장관 미국 버지니아주 해군 수상전센터 무기체계 개발 현황 한국 국방부 장관 방문 이번 처음 장관 방문 미국 제안 것으로 장관 한미 외교 국방장관 회의 참\n",
      "\n",
      "doc=9, num words=32, 서울 연합뉴스 기자 19일 서울 시내 경찰관 사제총기 발사 경찰관 숨지게 폭행 용의자 인근 상인들 것으로 확인 왼쪽 사진 신발 주인 직원 오른쪽 사진 매운탕 주인 시민 3명 경찰 용의자 검거 2016\n",
      "\n",
      "doc=10, num words=134, 서울경찰청장 병원 방문 유족들 위로 서울 연합뉴스 박경 기자 19일 사제 총기범 총탄 김창 54 경위 시신이 안치 서울 도봉구 한일병원 유족들 슬픔 감추지 유족들 안치 경위 시신 보고 오열 외아들 아내 경위 침상 떠나지 부인 오열 쓰러져 의료진 치료 받기 것으로 병원 유가족 물론 동료 경찰들 소식 달려 애도 동료 경찰들 평소 의협심 후임 먼저 경찰 경위 사\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(Corpus(tokenized_corpus_fname)):\n",
    "    if i <= 5: continue\n",
    "    if i > 10: break\n",
    "    print('doc=%d, num words=%d, %s\\n' % (i, len(doc.split()), doc[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorpusbasedKeywordExtractor를 만들 때, 애초에 키워드 후보가 될 수 있는 단어를 minimum term frequency (min_tf)와 minimum document frequency (min_df)로 필터링 할 수 있도록 하였습니다. 키워드의 후보들은 모두 min_tf, min_df 이상이 되는 단어들로 한정됩니다. \n",
    "\n",
    "tokenize는 텍스트 형식의 corpus에서 단어를 추출하기 위한 tokenizer입니다. 기본값은 띄어쓰기입니다만, KoNLPy의 nouns()나 pos()를 이용할 수도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done 34572 terms, 30091 docs, memory = 0.438 Gb6 Gb\n"
     ]
    }
   ],
   "source": [
    "from soykeyword.proportion import CorpusbasedKeywordExtractor\n",
    "\n",
    "corpusbased_extractor = CorpusbasedKeywordExtractor(\n",
    "    min_tf=20,\n",
    "    min_df=2,\n",
    "    tokenize=lambda x:x.strip().split(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "corpusbased_extractor.train(Corpus(tokenized_corpus_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어가 corpus에서 몇 번 등장했는지 빈도를 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근혜 1445\n",
      "문재인 1010\n",
      "최순실 1318\n",
      "아이오아이 270\n",
      "트와이스 655\n",
      "군사 170\n",
      "외교 881\n"
     ]
    }
   ],
   "source": [
    "for word in ['박근혜', '문재인', '최순실', '아이오아이', '트와이스', '군사', '외교']:\n",
    "    print(word, corpusbased_extractor.frequency(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "존재하지 않는 단어는 빈도수가 0으로 출력됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusbased_extractor.frequency('lovit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'아이오아이'가 포함된 문서 번호 (텍스트 파일에서의 line number)를 가지고 올 수도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6884, 6897, 6956, 7338, 7345, 7582, 8011, 8053, 9180, 9228]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = corpusbased_extractor.get_document_index('아이오아이')\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "키워드를 선택하는 방법은 아래와 같이 두가지 입니다. \n",
    "\n",
    "    corpusbased_extractor.extract_from_word('아이오아이', min_score=0.8, min_frequency=100)\n",
    "    corpusbased_extractor.extract_from_docs(documents, min_score=0.8, min_frequency=100)\n",
    "    \n",
    "extract_from_word(aspect_word)는 기준점이 되는 단어를 넣으면 min_score, min_frequency 이상이 되는 단어들 을 선택합니다. target document는 aspect_word가 포함된 문서 집합이며, reference document는 aspect_word가 포함되지 않은 문서 집합입니다. \n",
    "\n",
    "'아이오아이'가 포함된 문서가 target document이기 때문에 '아이오아이' 단어는 score가 반드시 1.0입니다. 그 외의 키워드에서 '엠카운트다운'이나 '걸크러쉬', '타이틀곡' 등이 키워드로 선택된 것으로 보아 Mnet의 엠카운트다운에 출현한 내용에 대한 기사들이 있던 것으로 추정됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word='아이오아이', frequency=270, score=1.0),\n",
       " KeywordScore(word='엠카운트다운', frequency=221, score=0.997897148491129),\n",
       " KeywordScore(word='펜타곤', frequency=104, score=0.9936420169665052),\n",
       " KeywordScore(word='잠깐', frequency=162, score=0.9931809154109712),\n",
       " KeywordScore(word='엠넷', frequency=125, score=0.9910325251765126),\n",
       " KeywordScore(word='걸크러쉬', frequency=111, score=0.9904705029926091),\n",
       " KeywordScore(word='타이틀곡', frequency=311, score=0.987384461584851),\n",
       " KeywordScore(word='코드', frequency=105, score=0.9871835929954923),\n",
       " KeywordScore(word='본명', frequency=105, score=0.9863934667369743),\n",
       " KeywordScore(word='엑스', frequency=101, score=0.9852544036088814)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = corpusbased_extractor.extract_from_word(\n",
    "    '아이오아이',\n",
    "    min_score=0.8,\n",
    "    min_frequency=100\n",
    ")\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹은 직접 target document의 document index를 입력할 수도 있습니다. 앞서서 '아이오아이'가 포함된 문서아이디를 documents로 선택하였기 때문에 아래의 키워드 추출 결과는 위와 동일합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word='아이오아이', frequency=270, score=1.0),\n",
       " KeywordScore(word='엠카운트다운', frequency=221, score=0.997897148491129),\n",
       " KeywordScore(word='펜타곤', frequency=104, score=0.9936420169665052),\n",
       " KeywordScore(word='잠깐', frequency=162, score=0.9931809154109712),\n",
       " KeywordScore(word='엠넷', frequency=125, score=0.9910325251765126),\n",
       " KeywordScore(word='걸크러쉬', frequency=111, score=0.9904705029926091),\n",
       " KeywordScore(word='타이틀곡', frequency=311, score=0.987384461584851),\n",
       " KeywordScore(word='코드', frequency=105, score=0.9871835929954923),\n",
       " KeywordScore(word='본명', frequency=105, score=0.9863934667369743),\n",
       " KeywordScore(word='엑스', frequency=101, score=0.9852544036088814)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = corpusbased_extractor.extract_from_docs(\n",
    "    documents,\n",
    "    min_score=0.8,\n",
    "    min_frequency=100\n",
    ")\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외의 단어들에 대해서도 키워드를 선택하면 아래와 같습니다. 아래는 키워드를 <단어, (빈도수, score)>로 표현한 것들입니다. 2016-10-20에는 박근혜게이트가 언론에 보도되기 시작하는 시기입니다. 당시에 비선실세와 연설문 사건들이 뉴스에 등장하기 시작했습니다. 이러한 단어들이 실제로 추출됨을 확인할 수 있습니다. \n",
    "\n",
    "우리는 extract_from_word(aspect_word)의 결과를 다르게 해석할 수 있습니다. 한 단어를 기준으로 target document를 잡은 뒤 키워드를 선택하는 것은 aspect_word가 들어간 문서를 잘 구분하는 features 이면서도 유독 aspect_word와 함께 등장하는 단어라는 뜻입니다. 즉, aspect_word의 연관어로 해석할 수도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect word = 박근혜 (1445)\n",
      " 박근혜 (1445, 1.00)  -  수석비서관회의 (208, 1.00)  -    재단들 (152, 1.00)  -    연설문 (204, 0.99)\n",
      "  누구라 (178, 0.99)  -   불법행위 (240, 0.99)  -     퇴임 (188, 0.98)  -     엄정 (388, 0.98)\n",
      " 창조경제 (226, 0.98)  -    처벌받 (227, 0.98)  -     미르 (604, 0.98)  -  스포츠재단 (676, 0.97)\n",
      "더블루케이 (194, 0.97)  -     최씨 (695, 0.97)  -    재단 (1690, 0.97)  -  자유학기제 (201, 0.97)\n",
      " 비선실세 (219, 0.97)  -   최순실씨 (520, 0.97)  -   미르재단 (247, 0.96)  -    게이트 (303, 0.96)\n",
      " 대통령 (5682, 0.96)  -     모녀 (223, 0.96)  -   행복교육 (227, 0.95)  -     실세 (309, 0.95)\n",
      "   비선 (288, 0.95)  -   최순실 (1318, 0.95)  -    의혹 (3602, 0.95)  -     고양 (278, 0.95)\n",
      "   국정 (185, 0.94)  -   청와대 (2112, 0.94)  -    지지층 (151, 0.94)  -    킨텍스 (332, 0.94)\n",
      "   체육 (221, 0.94)  -     재계 (152, 0.93)  -     민생 (164, 0.93)  -  2002년 (186, 0.93)\n",
      "   정권 (596, 0.93)  -     가중 (175, 0.93)  -     유용 (359, 0.93)  -    전경련 (348, 0.93)\n",
      "   주재 (459, 0.93)  -    국민들 (441, 0.93)  -     백승 (216, 0.92)  -    갤러리 (271, 0.92)\n",
      "  기업들 (808, 0.92)  -    지지율 (336, 0.92)  -     확산 (800, 0.91)  -    철저히 (327, 0.91)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 문재인 (1010)\n",
      " 문재인 (1010, 1.00)  -    색깔론 (160, 0.99)  -    김만복 (240, 0.99)  -     기권 (532, 0.99)\n",
      "북한인권결의안 (324, 0.99)  -   회고록 (1190, 0.99)  -    송민순 (821, 0.99)  -  외교통상부 (184, 0.99)\n",
      "  노무현 (303, 0.99)  -   국정원장 (795, 0.99)  -    이병호 (370, 0.99)  -     쪽지 (203, 0.99)\n",
      "인권결의안 (168, 0.98)  -    성북구 (153, 0.98)  -    안철수 (201, 0.98)  -     표결 (231, 0.98)\n",
      "  정보위 (340, 0.97)  -   제3지대 (172, 0.97)  -     야권 (450, 0.97)  -  정보위원회 (153, 0.97)\n",
      "   진실 (498, 0.97)  -     개헌 (332, 0.97)  -   사무총장 (211, 0.97)  -    더민주 (355, 0.96)\n",
      "   간사 (473, 0.96)  -    브리핑 (565, 0.96)  -  2007년 (741, 0.96)  -    결의안 (177, 0.96)\n",
      "   남북 (199, 0.96)  -    국정원 (797, 0.95)  -     공세 (450, 0.95)  -  새누리당 (2151, 0.95)\n",
      "   탈당 (646, 0.95)  -    원장 (1296, 0.95)  -     유엔 (738, 0.95)  -   과학기술 (248, 0.95)\n",
      "   파문 (235, 0.95)  -     여당 (471, 0.95)  -   비서실장 (445, 0.95)  -     공방 (278, 0.94)\n",
      "   왜곡 (179, 0.94)  -     국정 (185, 0.94)  -     찬성 (325, 0.94)  -  더불어민주당 (1980, 0.94)\n",
      "  지지율 (336, 0.94)  -     여야 (477, 0.94)  -    지지층 (151, 0.94)  -     전제 (170, 0.94)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 최순실 (1318)\n",
      " 최순실 (1318, 1.00)  -    게이트 (303, 1.00)  -    정유라 (329, 1.00)  -    연설문 (204, 0.99)\n",
      "   모녀 (223, 0.99)  -     비선 (288, 0.99)  -  더블루케이 (194, 0.98)  -     실세 (309, 0.98)\n",
      "스포츠재단 (676, 0.98)  -     최씨 (695, 0.98)  -    최경희 (223, 0.98)  -   이화여대 (651, 0.98)\n",
      "   특혜 (532, 0.98)  -   미르재단 (247, 0.98)  -     학점 (191, 0.98)  -   비선실세 (219, 0.98)\n",
      "   이대 (419, 0.97)  -     미르 (604, 0.97)  -    재단 (1690, 0.97)  -   정유라씨 (200, 0.97)\n",
      "   엄정 (388, 0.97)  -     사퇴 (463, 0.96)  -    의혹 (3602, 0.96)  -    누구라 (178, 0.96)\n",
      "   사임 (245, 0.96)  -    교수들 (183, 0.96)  -     입학 (356, 0.96)  -   창조경제 (226, 0.96)\n",
      " 최순실씨 (520, 0.95)  -  수석비서관회의 (208, 0.95)  -    총장 (1215, 0.95)  -    문체부 (268, 0.95)\n",
      "   국정 (185, 0.95)  -    색깔론 (160, 0.95)  -     침묵 (223, 0.95)  -   불법행위 (240, 0.95)\n",
      "   모금 (238, 0.95)  -    재단들 (152, 0.95)  -    처벌받 (227, 0.95)  -     본관 (204, 0.95)\n",
      "   비리 (427, 0.94)  -   청와대 (2112, 0.94)  -   박근혜 (1445, 0.94)  -     퇴임 (188, 0.94)\n",
      "   개입 (473, 0.93)  -    설립 (1522, 0.93)  -    전경련 (348, 0.93)  -     더블 (225, 0.93)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 아이오아이 (270)\n",
      "아이오아이 (270, 1.00)  -  엠카운트다운 (221, 1.00)  -     잠깐 (162, 0.99)  -   타이틀곡 (311, 0.99)\n",
      "방탄소년단 (638, 0.98)  -     키미 (297, 0.98)  -     보컬 (155, 0.98)  -   에이핑크 (237, 0.98)\n",
      "   유정 (161, 0.98)  -    파워풀 (152, 0.98)  -     형은 (311, 0.98)  -   프로듀스 (185, 0.98)\n",
      "  샤이니 (299, 0.98)  -    불독 (1212, 0.98)  -    다이아 (182, 0.98)  -     음반 (204, 0.98)\n",
      "   컴백 (536, 0.98)  -     세이 (267, 0.98)  -     순위 (259, 0.98)  -    콘셉트 (320, 0.98)\n",
      "  멤버들 (504, 0.98)  -     소라 (262, 0.97)  -    무대 (1332, 0.97)  -     발랄 (250, 0.97)\n",
      "   언니 (172, 0.97)  -     진영 (304, 0.97)  -     뮤직 (195, 0.97)  -   서바이벌 (203, 0.97)\n",
      "   싱글 (432, 0.97)  -     당당 (242, 0.97)  -   걸그룹 (1060, 0.96)  -    사운드 (189, 0.96)\n",
      "   각오 (168, 0.96)  -     강렬 (352, 0.96)  -    101 (341, 0.96)  -     실감 (167, 0.96)\n",
      " 쇼케이스 (549, 0.95)  -     작사 (230, 0.95)  -    1위 (1357, 0.95)  -    데뷔 (1365, 0.95)\n",
      " 미니앨범 (197, 0.95)  -     멤버 (624, 0.95)  -   프로듀서 (223, 0.95)  -     신곡 (400, 0.94)\n",
      "   신인 (328, 0.94)  -     일산 (194, 0.94)  -  뉴스1스타 (357, 0.94)  -     롤링 (391, 0.94)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 트와이스 (655)\n",
      " 트와이스 (655, 1.00)  -   가온차트 (191, 1.00)  -   스트리밍 (329, 1.00)  -   미니앨범 (197, 1.00)\n",
      "  1주년 (201, 1.00)  -     티저 (289, 0.99)  -    두번째 (201, 0.99)  -  뮤직비디오 (553, 0.99)\n",
      "  타이틀 (270, 0.99)  -     누적 (799, 0.99)  -    아이돌 (301, 0.98)  -    유튜브 (473, 0.98)\n",
      "   컴백 (536, 0.98)  -     0시 (190, 0.98)  -    1위 (1357, 0.98)  -   프로모션 (198, 0.98)\n",
      "   음반 (204, 0.98)  -     1억 (415, 0.98)  -   타이틀곡 (311, 0.97)  -     93 (181, 0.97)\n",
      "  데뷔 (1365, 0.97)  -     코너 (239, 0.97)  -     맞은 (316, 0.97)  -     맞이 (293, 0.97)\n",
      " 걸그룹 (1060, 0.97)  -     한류 (297, 0.97)  -     앞둔 (370, 0.97)  -    판매량 (231, 0.96)\n",
      "2016년 (1337, 0.96)  -     대만 (251, 0.96)  -   페스티벌 (334, 0.96)  -     팬들 (999, 0.96)\n",
      "   신곡 (400, 0.96)  -  아이오아이 (270, 0.96)  -     잠실 (188, 0.95)  -   24일 (1136, 0.95)\n",
      "  콘서트 (463, 0.95)  -     입증 (297, 0.95)  -    6개월 (382, 0.95)  -     음원 (318, 0.95)\n",
      "   4월 (823, 0.95)  -    콘셉트 (320, 0.95)  -     축하 (189, 0.95)  -     73 (246, 0.94)\n",
      " 블랙핑크 (190, 0.94)  -    번째 (1158, 0.94)  -     돌파 (569, 0.94)  -     문구 (152, 0.94)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 군사 (170)\n",
      "   군사 (170, 1.00)  -    내구성 (174, 0.98)  -    테스트 (534, 0.98)  -     전자 (669, 0.97)\n",
      "  국방부 (462, 0.96)  -     평택 (272, 0.96)  -    군사적 (179, 0.96)  -     국방 (399, 0.96)\n",
      "   해군 (306, 0.96)  -    협의체 (408, 0.96)  -     품질 (461, 0.96)  -    휴대폰 (213, 0.96)\n",
      "확장억제전략협의체 (150, 0.96)  -   남중국해 (188, 0.95)  -     전투 (196, 0.95)  -     시험 (683, 0.95)\n",
      "   미군 (152, 0.95)  -     성능 (309, 0.95)  -    고위급 (189, 0.95)  -     억제 (324, 0.95)\n",
      " 확장억제 (521, 0.95)  -     부장 (178, 0.95)  -     저자 (222, 0.95)  -     대북 (409, 0.94)\n",
      " 스마트폰 (996, 0.94)  -   공동성명 (155, 0.94)  -     카터 (208, 0.94)  -     독도 (217, 0.94)\n",
      "   분쟁 (247, 0.94)  -    핵무기 (227, 0.93)  -     외교 (881, 0.93)  -    전략적 (178, 0.93)\n",
      "   회담 (180, 0.93)  -     양국 (971, 0.93)  -     한미 (995, 0.93)  -     3층 (164, 0.93)\n",
      "  연설문 (204, 0.93)  -   국방장관 (504, 0.93)  -   북한인권 (171, 0.93)  -     충격 (477, 0.93)\n",
      "  워싱턴 (557, 0.93)  -    한반도 (416, 0.93)  -     드론 (196, 0.92)  -     수백 (214, 0.92)\n",
      "   신설 (444, 0.92)  -     통과 (493, 0.92)  -    생산 (1186, 0.92)  -    김정은 (479, 0.92)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 외교 (881)\n",
      "   외교 (881, 1.00)  -  확장억제전략협의체 (150, 1.00)  -   확장억제 (521, 1.00)  -   공동성명 (155, 1.00)\n",
      " 국방장관 (504, 1.00)  -     케리 (401, 1.00)  -    고위급 (189, 1.00)  -     카터 (208, 1.00)\n",
      "   한미 (995, 1.00)  -    윤병세 (298, 0.99)  -    핵무기 (227, 0.99)  -    협의체 (408, 0.99)\n",
      "  군사적 (179, 0.99)  -    워싱턴 (557, 0.99)  -     국방 (399, 0.99)  -    국무부 (233, 0.99)\n",
      "   양국 (971, 0.99)  -   국무장관 (306, 0.99)  -    외교부 (573, 0.98)  -     사드 (251, 0.98)\n",
      "  재확인 (189, 0.98)  -     대북 (409, 0.98)  -    안보리 (195, 0.98)  -    한반도 (416, 0.98)\n",
      "   상시 (258, 0.98)  -     직면 (211, 0.98)  -   두테르테 (377, 0.97)  -   미사일 (1335, 0.97)\n",
      "   억제 (324, 0.97)  -     위협 (749, 0.97)  -    압도적 (190, 0.97)  -    핵실험 (172, 0.97)\n",
      " 남중국해 (188, 0.97)  -     공약 (274, 0.97)  -   국제사회 (239, 0.97)  -     결의 (258, 0.96)\n",
      "   배치 (805, 0.96)  -     북핵 (156, 0.96)  -   정상회담 (174, 0.96)  -   북한인권 (171, 0.96)\n",
      "   도발 (422, 0.96)  -     제재 (470, 0.96)  -     압박 (483, 0.96)  -    국방부 (462, 0.96)\n",
      "   동맹 (267, 0.96)  -     회담 (180, 0.95)  -    북한 (4132, 0.95)  -    무수단 (533, 0.95)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for word in ['박근혜', '문재인', '최순실', '아이오아이', '트와이스', '군사', '외교']:\n",
    "\n",
    "    keywords = corpusbased_extractor.extract_from_word(\n",
    "        word, min_score=0.8,\n",
    "        min_frequency=150\n",
    "    )\n",
    "\n",
    "    keywords = keywords[:48]\n",
    "\n",
    "    word_frequency = corpusbased_extractor.frequency(word)\n",
    "    print('Aspect word = %s (%d)' % (word, word_frequency))\n",
    "\n",
    "    def in_a_line(subkeywords):\n",
    "        def tuple_to_strf(keyword):\n",
    "            return '%s (%d, %.2f)' % keyword        \n",
    "        strf = [tuple_to_strf(keyword) for keyword in subkeywords]\n",
    "        strf = ['%17s' % s for s in strf]\n",
    "        return '  -  '.join(strf)\n",
    "\n",
    "    for i in range(12):\n",
    "        subkeywords = keywords[4*i:4*i+4]\n",
    "        line = in_a_line(subkeywords)\n",
    "        print(line)\n",
    "\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 머신러닝 알고리즘에 적용하기 위해서 sparse matrix 형식으로 데이터를 저장해둔 경우들도 있습니다. 이때에도 키워드 추출이 용이하도록 MatrixbasedKeywordExtractor를 만들어두었습니다. Interface나 작동 방식은 위와 동일합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30091, 9774)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0.001)\n",
    "x = vectorizer.fit_transform(Corpus(tokenized_corpus_fname))\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn의 CountVectorizer를 이용하여 term frequency matrix; x를 만들고, {word:index}의 dictionary와 [word, ...]의 list of str을 만들어두었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = vectorizer.vocabulary_\n",
    "index2word = sorted(\n",
    "    vectorizer.vocabulary_,\n",
    "    key=lambda x:vectorizer.vocabulary_[x]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix 형식이기 때문에 tokenize는 필요없습니다. 그 외의 parameters는 동일합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixbasedKeywordExtractor trained\n"
     ]
    }
   ],
   "source": [
    "from soykeyword.proportion import MatrixbasedKeywordExtractor\n",
    "\n",
    "matrixbased_extractor = MatrixbasedKeywordExtractor(\n",
    "    min_tf=20,\n",
    "    min_df=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "matrixbased_extractor.train(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrixbased_extractor.train은 두 가지 argument를 필요로 합니다. \n",
    "\n",
    "    matrixbased_extractor.train(x, index2word=None)\n",
    "    \n",
    "만약 index2word가 입력되지 않으면 word index로만 키워드가 선택됩니다. 5537은 '아이오아이'입니다. matrixbased_extractor.train(5537)의 결과는 아래처럼 word index로 출력됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word=5537, frequency=270, score=1.0),\n",
       " KeywordScore(word=5880, frequency=221, score=0.9978307775631691),\n",
       " KeywordScore(word=8976, frequency=104, score=0.9934422266805437),\n",
       " KeywordScore(word=7126, frequency=162, score=0.9929667382454291),\n",
       " KeywordScore(word=5879, frequency=125, score=0.9907514986652862),\n",
       " KeywordScore(word=1103, frequency=111, score=0.99017203825805),\n",
       " KeywordScore(word=8721, frequency=311, score=0.9869906112674688),\n",
       " KeywordScore(word=8651, frequency=105, score=0.9867835556082788),\n",
       " KeywordScore(word=4035, frequency=105, score=0.98596911773225),\n",
       " KeywordScore(word=5869, frequency=101, score=0.9847950780631249)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = matrixbased_extractor.extract_from_word(\n",
    "    5537,\n",
    "    min_score=0.8,\n",
    "    min_frequency=100\n",
    ")\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 앞서 만든 index2word를 이용하여 decoding하면 아래와 같은 결과가 나옵니다. 이는 CorpusbasedKeywordExtractor의 결과와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=아이오아이, frequency=270, score=1.000\n",
      "word=엠카운트다운, frequency=221, score=0.998\n",
      "word=펜타곤, frequency=104, score=0.993\n",
      "word=잠깐, frequency=162, score=0.993\n",
      "word=엠넷, frequency=125, score=0.991\n",
      "word=걸크러쉬, frequency=111, score=0.990\n",
      "word=타이틀곡, frequency=311, score=0.987\n",
      "word=코드, frequency=105, score=0.987\n",
      "word=본명, frequency=105, score=0.986\n",
      "word=엑스, frequency=101, score=0.985\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords[:10]:\n",
    "\n",
    "    word = index2word[keyword.word]\n",
    "    frequency = keyword.frequency\n",
    "    score = keyword.score\n",
    "\n",
    "    print('word=%s, frequency=%d, score=%.3f' % (\n",
    "        word, frequency, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train()에서 index2word를 입력하지 않았기 때문에 extract_from_word()에 str 형식의 단어를 입력하면 index2word를 먼저 넣으라는 Exception이 발생합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to insert str word, you should trained index2word first\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    keywords = matrixbased_extractor.extract_from_word(\n",
    "        '아이오아이',\n",
    "        min_score=0.8,\n",
    "        min_frequency=100\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 train단계에서 아래와 같이 index2word; list of str(word)를 넣어주면 5537과 같은 word index를 입력하여도, '아이오아이'와 같은 단어를 입력하여도 모두 아래와 같이 단어로 표현된 키워드 추출 결과가 return 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixbasedKeywordExtractor trained\n"
     ]
    }
   ],
   "source": [
    "matrixbased_extractor_w_indexer = MatrixbasedKeywordExtractor(\n",
    "    min_tf=20,\n",
    "    min_df=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "matrixbased_extractor.train(x, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word='아이오아이', frequency=270, score=1.0),\n",
       " KeywordScore(word='엠카운트다운', frequency=221, score=0.9978307775631691),\n",
       " KeywordScore(word='펜타곤', frequency=104, score=0.9934422266805437),\n",
       " KeywordScore(word='잠깐', frequency=162, score=0.9929667382454291),\n",
       " KeywordScore(word='엠넷', frequency=125, score=0.9907514986652862),\n",
       " KeywordScore(word='걸크러쉬', frequency=111, score=0.99017203825805),\n",
       " KeywordScore(word='타이틀곡', frequency=311, score=0.9869906112674688),\n",
       " KeywordScore(word='코드', frequency=105, score=0.9867835556082788),\n",
       " KeywordScore(word='본명', frequency=105, score=0.98596911773225),\n",
       " KeywordScore(word='엑스', frequency=101, score=0.9847950780631249)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = matrixbased_extractor.extract_from_word(\n",
    "    5537,\n",
    "    min_score=0.8,\n",
    "    min_frequency=100\n",
    ")\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word='아이오아이', frequency=270, score=1.0),\n",
       " KeywordScore(word='엠카운트다운', frequency=221, score=0.9978307775631691),\n",
       " KeywordScore(word='펜타곤', frequency=104, score=0.9934422266805437),\n",
       " KeywordScore(word='잠깐', frequency=162, score=0.9929667382454291),\n",
       " KeywordScore(word='엠넷', frequency=125, score=0.9907514986652862),\n",
       " KeywordScore(word='걸크러쉬', frequency=111, score=0.99017203825805),\n",
       " KeywordScore(word='타이틀곡', frequency=311, score=0.9869906112674688),\n",
       " KeywordScore(word='코드', frequency=105, score=0.9867835556082788),\n",
       " KeywordScore(word='본명', frequency=105, score=0.98596911773225),\n",
       " KeywordScore(word='엑스', frequency=101, score=0.9847950780631249)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = matrixbased_extractor.extract_from_word(\n",
    "    '아이오아이',\n",
    "    min_score=0.8,\n",
    "    min_frequency=100)\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
